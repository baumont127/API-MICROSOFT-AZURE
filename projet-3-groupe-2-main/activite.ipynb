{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activité - Face detection - Emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bienvenue aux Oscars Simplon 2022 !\n",
    "\n",
    "**Objectif**\n",
    "\n",
    "Dans cette activité, vous devrez montrer vos talents d'acteur en simulant certaines émotions : la joie, la tristesse, la colère et la surprise.\n",
    "\n",
    "Pour cela, vous allez utiliser la fonctionnalité detect de Face API d'Azure. Pour plus d'informations sur Face API : https://docs.microsoft.com/en-us/azure/cognitive-services/face/overview \n",
    "\n",
    "La fonctionnalité detect permet de détecter un visage sur une image et d'en extraire certains attributs (âge, genre, émotions, position du visage sur l'image...). Ce qui nous intéresse particulièrement dans le cadre de cette activité est d'extraire l'émotion ainsi que la position du visage sur l'image.\n",
    "Pour plus d'informations sur la fonctionnalité detect : https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236\n",
    "\n",
    "**Tâches à réaliser**\n",
    "\n",
    "Pour chaque émotion à simuler, vous devrez donc :\n",
    "\n",
    "- Vous prendre en photo à l'aide de la webcam de votre ordinateur et sauvegarder la photo dans un fichier jpg portant le nom de l'émotion à simuler (le code python complet vous est fourni pour cette étape, il vous faudra juste modifier le nom du fichier jpg pour chaque émotion)\n",
    "- Détecter le visage sur l'image\n",
    "- Extraire l'émotion et le score de confiance correspondant\n",
    "- Extraire les coordonnées du rectangle délimitant le visage sur l'image (= position du visage sur l'image)\n",
    "- Afficher l'image avec le rectangle délimitant le visage annoté de l'émotion extraite et du score de confiance correspondant\n",
    "\n",
    "**Validation**\n",
    "\n",
    "Si l'émotion retournée par l'API correspond effectivement à l'émotion simulée, vous pouvez passer à l'émotion suivante. Sinon, reprenez vous en photo jusqu'à obtenir l'émotion correspondante.\n",
    "\n",
    "Pour valider votre nomination aux Oscars Simplon 2022, il faudra nous présenter les 4 fichiers jpg contenant vos photos simulant les 4 émotions. Nous vérifierons ensuite la validité des résultats.\n",
    "\n",
    "**Note**\n",
    "\n",
    "Vous devez compléter le code à trous fourni.\n",
    "Vous trouverez en commentaires dans le code les instructions pour le compléter sous la forme : ### Instructions ###."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche d'informations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Quels sont les 3 services proposés dans le module vision des cognitive services d'Azure ?\n",
    "\n",
    "2) Quelles sont les principales fonctionnalités liées à la reconnaissance des visages diponibles ?\n",
    "\n",
    "3) Quelles sont les principales fonctionnalités disponibles dans le service de computer vision ?\n",
    "\n",
    "4) Citez un ou plusieurs cas d'usages pour la reconnaissance émotionnelle faciale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables d'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un fichier .env dans votre répertoire de travail.\n",
    "Insérer dans ce fichier les variables d'environnement correspondant à la clé d'API et au Endpoint :\n",
    "\n",
    "COG_SERVICE_ENDPOINT=...\n",
    "COG_SERVICE_KEY=..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour accéder aux variables d'environnement\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Pour accéder à la webcam de l'ordinateur et se prendre en photo\n",
    "import cv2 \n",
    "\n",
    "# Pour effectuer les requêtes à l'API\n",
    "import requests\n",
    "\n",
    "# Pour afficher l'image, dessiner le rectangle délimitant le visage et annoter avec l'émotion\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération de la clé d'API et du endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "cog_endpoint = os.getenv(\"COG_SERVICE_ENDPOINT\")\n",
    "cog_key = os.getenv(\"COG_SERVICE_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prise de photo avec la webcam\n",
    "\n",
    "C'est le moment de montrer vos talents d'acteur !\n",
    "\n",
    "Pour rappel, vous devez simuler les émotions suivantes : la joie, la tristesse, la colère et la surprise.\n",
    "\n",
    "Libre à vous de commencer par celle que vous voulez. Une fois l'émotion bien simulée, prenez vous en photo en appuyant sur la touche 's' du clavier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = cv2. waitKey(1)\n",
    "webcam = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    try:\n",
    "        check, frame = webcam.read()\n",
    "        cv2.imshow(\"Capturing\", frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('s'):\n",
    "            \n",
    "            ### Entrez dans filename le nom du fichier .jpg correspondant à l'émotion que vous simulez (par exemple : happy.jpg) ###\n",
    "            cv2.imwrite(filename='...', img=frame)\n",
    "            webcam.release()\n",
    "            cv2.waitKey(1650)\n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"Processing image...\")\n",
    "            \n",
    "            print(\"Image saved!\")\n",
    "        \n",
    "            break\n",
    "        elif key == ord('q'):\n",
    "            print(\"Turning off camera.\")\n",
    "            webcam.release()\n",
    "            print(\"Camera off.\")\n",
    "            print(\"Program ended.\")\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        \n",
    "    except(KeyboardInterrupt):\n",
    "        print(\"Turning off camera.\")\n",
    "        webcam.release()\n",
    "        print(\"Camera off.\")\n",
    "        print(\"Program ended.\")\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détection du visage et extraction de l'émotion\n",
    "\n",
    "Documentation API REST detect : https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(key, endpoint, img):\n",
    "    \"\"\"\n",
    "    Détecte un ou plusieurs visages sur une image et extrait l'émotion avec son score de confiance correspondant\n",
    "    Affiche l'image avec le rectangle délimitant le visage et une annotation de l'émotion détectée avec son score de confiance\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : str\n",
    "        Clé d'API\n",
    "    endpoint : str\n",
    "        Endpoint de l'API\n",
    "    img : str\n",
    "        Nom du fichier jpg correspondant à l'image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Liste contenant l'émotion, le score de confiance correspondant et les coordonnées du rectangle délimitant le visage détecté\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### Construire l'API URL endpoint pour le Face API service d'Azure ###\n",
    "    url = ...\n",
    "    \n",
    "    ### Renseigner la clé d'API dans les headers ###\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/octet-stream\",\n",
    "        \"Ocp-Apim-Subscription-Key\": ...,\n",
    "    }\n",
    "\n",
    "    with open(img,'rb') as image:\n",
    "        \n",
    "        ### Renseigner les paramètres nécessaires pour permettre l'extraction de l'émotion sur le visage ###\n",
    "        params = {\n",
    "        \n",
    "        }\n",
    "\n",
    "        ### Faire une requête POST en renseignant les paramètres url, data, params et headers avec les bonnes valeurs ###\n",
    "        response = ...\n",
    "\n",
    "        response.raise_for_status()\n",
    "    \n",
    "        data = response.json()\n",
    "\n",
    "        \n",
    "        ### Récupérer dans une liste : l'émotion avec le score de confiance le plus élevé ainsi que les coordonnées du rectangle délimitant le visage détecté ###\n",
    "        ### Nous vous suggérons de stocker les coordonnées du rectangle dans un dictionnaire ###\n",
    "        face_att = []\n",
    "        \n",
    "        for face in data:\n",
    "            rect = {}\n",
    "            emotion = ...\n",
    "            score_emotion = ...\n",
    "            \n",
    "            face_att.append((emotion, score_emotion, rect))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Affichage de l'image et dessin du rectangle avec l'émotion correspondant au visage en annotation\n",
    "        \n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        plt.axis('off')\n",
    "        image = Image.open(img)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        color = 'red'\n",
    "\n",
    "        \n",
    "        for face in face_att:\n",
    "\n",
    "            ### Récupérer les attributs du visage ###\n",
    "            emotion = ...\n",
    "            score_emotion = ...\n",
    "            left = ...\n",
    "            top = ...\n",
    "            width = ...\n",
    "            height = ...\n",
    "            \n",
    "            # Définition et affichage du rectangle sur l'image\n",
    "            bounding_box = ((left,top), (left + width, top + height))    \n",
    "            draw.rectangle(bounding_box, outline=color, width=4)\n",
    "                            \n",
    "            ### Annoter l'image avec l'émotion et le score ### \n",
    "            annotation = f\"\"           \n",
    "            plt.annotate(annotation, (left,top), backgroundcolor=color)\n",
    "\n",
    "        plt.imshow(image)\n",
    "\n",
    "        return face_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utiliser la fonction définie pour afficher l'image avec le rectangle délimitant le visage et l'émotion détectée ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ceux qui voudraient aller plus loin, il est possible d'extraire d'autres attributs du visage tels que l'âge, le genre, les accessoires..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : détection d'objets\n",
    "\n",
    "Ci-dessous un code à trous pour tester la détection d'objets. Vous pouvez vous prendre en photo avec un ou plusieurs objets pour tester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sdk service computer vision\n",
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from msrest.authentication import CognitiveServicesCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajouter les variables d'environnement dans le fichier .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "bonus_cog_endpoint = os.getenv(\"BONUS_COG_SERVICE_ENDPOINT\")\n",
    "bonus_cog_key = os.getenv(\"BONUS_COG_SERVICE_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prendre une photo avec un ou plusieurs objets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = cv2. waitKey(1)\n",
    "webcam = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    try:\n",
    "        check, frame = webcam.read()\n",
    "        \n",
    "        cv2.imshow(\"Capturing\", frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('s'): \n",
    "            cv2.imwrite(filename='saved_img.jpg', img=frame)\n",
    "            webcam.release()\n",
    "            img_new = cv2.imread('saved_img.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "            img_new = cv2.imshow(\"Captured Image\", img_new)\n",
    "            cv2.waitKey(1650)\n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"Processing image...\")\n",
    "            img_ = cv2.imread('saved_img.jpg', cv2.IMREAD_ANYCOLOR)\n",
    "            \n",
    "            print(\"Image saved!\")\n",
    "        \n",
    "            break\n",
    "        elif key == ord('q'):\n",
    "            print(\"Turning off camera.\")\n",
    "            webcam.release()\n",
    "            print(\"Camera off.\")\n",
    "            print(\"Program ended.\")\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        \n",
    "    except(KeyboardInterrupt):\n",
    "        print(\"Turning off camera.\")\n",
    "        webcam.release()\n",
    "        print(\"Camera off.\")\n",
    "        print(\"Program ended.\")\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter la définition de la fonction ci-dessous pour détecter les objets et afficher l'image annotée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection(img):\n",
    "    \n",
    "    with open(img,'rb') as image:\n",
    "        \n",
    "        ### à remplir les '...' ###\n",
    "        client = ComputerVisionClient(..., ...)\n",
    "\n",
    "        ### à remplir '***' ###\n",
    "        object_detection.detect_objects_results_remote = client.***(image)\n",
    "\n",
    "        \n",
    "    results = object_detection.detect_objects_results_remote\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    plt.axis('off')\n",
    "    image = Image.open(img)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    color = 'red'\n",
    "\n",
    "    for object in results.objects:\n",
    "\n",
    "\n",
    "            # Draw and annotate face\n",
    "            name = object.object_property\n",
    "            x = object.rectangle.x\n",
    "            y = object.rectangle.y\n",
    "            w = object.rectangle.w\n",
    "            h = object.rectangle.h\n",
    "\n",
    "\n",
    "            bounding_box = ((x,y), (x + w, y + h))\n",
    "\n",
    "\n",
    "            draw.rectangle(bounding_box, outline='green', width=4)\n",
    "\n",
    "\n",
    "            annotation = name\n",
    "\n",
    "\n",
    "            plt.annotate(annotation,(x, y), backgroundcolor='green')\n",
    "\n",
    "    # Save annotated image\n",
    "    plt.imshow(image)\n",
    "    outputfile = img\n",
    "    fig.savefig(outputfile)\n",
    "\n",
    "\n",
    "object_detection(\"saved_img.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9f0e8019c403664c3648e0839e9aaf166627bf278a81999a17fa18bfc8e77e3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
